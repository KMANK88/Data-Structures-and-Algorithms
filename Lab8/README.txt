Part (2)

a) My method work just great.
b) the run times are about linearithmic for the  heap and linear for the hash.
The bigger the files the longer it takes to make sure that they are part copies of each file. 

Run time for 100
.000065341

Run time for 10M 
40 seconds. 


Part (3)

You would need to grab the smallest element out of each array and then put it on the “new array” once you have an element that is bigger than the most recent element in the array you can get rid of it and go to the next one in the array. 

The run time will get longer the more sizes will be used in about linear time the bigger the sizes get. 